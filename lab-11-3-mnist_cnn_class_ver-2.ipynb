{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "( x_train, y_train ), ( x_test, y_test ) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = tf.cast( x_train / 255.0, tf.float32 )\n",
    "x_test = tf.cast( x_test / 255.0, tf.float32 )\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical( y_train, 10 )\n",
    "y_test = tf.keras.utils.to_categorical( y_test, 10 )\n",
    "print( x_train.shape, y_train.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__( self, name ):\n",
    "        self.name = name\n",
    "        self.__build_net()\n",
    "        \n",
    "    def __build_net( self ):\n",
    "        self.W1 = tf.Variable( tf.random.normal( [ 3, 3, 1, 32 ], stddev = 0.01 ) )\n",
    "        self.W2 = tf.Variable( tf.random.normal( [ 3, 3, 32, 64 ], stddev = 0.01 ) )\n",
    "        self.W3 = tf.Variable( tf.random.normal( [ 3, 3, 64, 128 ], stddev = 0.01 ) )\n",
    "        xavier = tf.keras.initializers.GlorotUniform()\n",
    "        self.W4 = tf.Variable( xavier( [ 128 * 4 * 4, 625 ] ) )\n",
    "        self.b4 = tf.Variable( tf.random.normal( [ 625 ] ) )\n",
    "        self.W5 = tf.Variable( xavier( [ 625, 10 ] ) )\n",
    "        self.b5 = tf.Variable( tf.random.normal( [ 10 ] ) )\n",
    "        \n",
    "    def __layer1( self, X, rate ):\n",
    "        l1 = tf.nn.conv2d( X, self.W1,\n",
    "                          strides = [ 1, 1, 1, 1 ], padding = 'SAME' )\n",
    "        l1 = tf.nn.relu( l1 )\n",
    "        l1 = tf.nn.max_pool( l1, ksize = [ 1, 2, 2, 1 ],\n",
    "                           strides = [ 1, 2, 2, 1 ], padding = 'SAME' )\n",
    "        return ( tf.nn.dropout( l1, rate ) )\n",
    "    \n",
    "    def __layer2( self, X, rate ):\n",
    "        l2 = tf.nn.conv2d( self.__layer1( X, rate ), self.W2,\n",
    "                          strides = [ 1, 1, 1, 1 ], padding = 'SAME' )\n",
    "        l2 = tf.nn.relu( l2 )\n",
    "        l2 = tf.nn.max_pool( l2, ksize = [ 1, 2, 2, 1 ],\n",
    "                           strides = [ 1, 2, 2, 1], padding = 'SAME' )\n",
    "        return ( tf.nn.dropout( l2, rate ) )\n",
    "    \n",
    "    def __layer3( self, X, rate ):\n",
    "        l3 = tf.nn.conv2d( self.__layer2( X, rate ), self.W3,\n",
    "                         strides = [ 1, 1, 1, 1 ], padding = 'SAME' )\n",
    "        l3 = tf.nn.relu( l3 )\n",
    "        l3 = tf.nn.max_pool( l3, ksize = [ 1, 2, 2, 1 ],\n",
    "                           strides = [ 1, 2, 2, 1 ], padding = 'SAME' )\n",
    "        return ( tf.nn.dropout( l3, rate ) )\n",
    "    \n",
    "    def __l3_flat( self, X, rate ):\n",
    "        return ( tf.reshape( self.__layer3( X, rate ), [ -1, 128 * 4 * 4 ] ) )\n",
    "                \n",
    "    def __layer4( self, X, rate ):\n",
    "        l4 = tf.nn.relu( tf.matmul( self.__l3_flat( X, rate ), self.W4 ) + self.b4 )\n",
    "        return ( tf.nn.dropout( l4, rate ) )\n",
    "                \n",
    "    def Logits( self, X, rate ):\n",
    "        X_img = tf.reshape( X, [ -1, 28, 28, 1 ] )        \n",
    "        return ( tf.matmul( self.__layer4( X_img, rate ), self.W5 ) + self.b5 )\n",
    "    \n",
    "    def Predict( self, x_test, rate = 0 ):\n",
    "        return ( self.Logits( x_test, rate ) )    \n",
    "    \n",
    "    def Cost( self, X, Y, rate ):\n",
    "        return ( tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits = self.Logits( X, rate ), labels = Y ) ) )\n",
    "    \n",
    "    def correct_prediction( self, X, Y, rate ):\n",
    "        return ( tf.equal( tf.argmax( self.Logits( X, rate ), 1 ),\n",
    "                         tf.argmax( Y, 1 ) ) )\n",
    "    \n",
    "    def get_accuracy( self, x_test, y_test, rate = 0 ):\n",
    "        return ( tf.reduce_mean( tf.cast( self.correct_prediction( x_test, y_test, rate ), tf.float32 ) ) )\n",
    "    \n",
    "    def train( self, X, Y, rate = 0.3 ):\n",
    "        loss = lambda: self.Cost( X, Y, rate )\n",
    "        tf.keras.optimizers.Adam( learning_rate ).minimize( loss,\n",
    "                                                          [ self.W1, self.W2, self.W3, self.W4, self.W5,\n",
    "                                                           self.b4, self.b5 ] )\n",
    "        \n",
    "        return loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = Model( 'm1' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Started!\n",
      "Epoch: 0001, cost: 0.449852079\n",
      "Epoch: 0002, cost: 0.100004002\n",
      "Epoch: 0003, cost: 0.077956147\n",
      "Epoch: 0004, cost: 0.072517671\n",
      "Epoch: 0005, cost: 0.072264887\n",
      "Epoch: 0006, cost: 0.066059336\n",
      "Epoch: 0007, cost: 0.069227867\n",
      "Epoch: 0008, cost: 0.066650793\n",
      "Epoch: 0009, cost: 0.071735561\n",
      "Epoch: 0010, cost: 0.073252872\n",
      "Epoch: 0011, cost: 0.079574421\n",
      "Epoch: 0012, cost: 0.075862400\n",
      "Epoch: 0013, cost: 0.072776549\n",
      "Epoch: 0014, cost: 0.082470112\n",
      "Epoch: 0015, cost: 0.077692308\n"
     ]
    }
   ],
   "source": [
    "print( 'Learning Started!' )\n",
    "for epoch in range( training_epochs ):\n",
    "    avg_cost = 0\n",
    "    total_batch = int( len( x_train ) / batch_size )\n",
    "    \n",
    "    start_batch, end_batch = 0, batch_size\n",
    "    for i in range( total_batch ):\n",
    "        batch_xs = x_train[ start_batch : end_batch ]\n",
    "        batch_ys = y_train[ start_batch : end_batch ]\n",
    "        \n",
    "        c = m1.train( batch_xs, batch_ys)\n",
    "        avg_cost += c / total_batch\n",
    "        \n",
    "        start_batch += batch_size\n",
    "        end_batch += batch_size\n",
    "        \n",
    "    print( 'Epoch: {:04d}, cost: {:.9f}'.format( epoch + 1, avg_cost ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9905\r\n"
     ]
    }
   ],
   "source": [
    "tf.print( 'Accuracy: ', m1.get_accuracy( x_test, y_test ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
